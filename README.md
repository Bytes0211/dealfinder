# Deal Finder

> AI-Powered Deal Hunting Autonomous Agent System

An intelligent multi-agent system that discovers online deals through RSS feeds, estimates prices using ensemble ML models, and delivers real-time notifications for high-value opportunities.

## ğŸ¯ Project Status

**Current Phase**: Phase 1 Complete âœ… - Infrastructure Deployed | Ready for Phase 2

**Infrastructure Live**: 47 AWS resources deployed via Terraform with full CI/CD and monitoring.

This repository contains comprehensive architecture documentation and a cost-optimized, production-ready infrastructure foundation.

## ğŸ—ï¸ Architecture Overview

Deal Finder is a **multi-agent AI system** built on AWS with Apache technologies:

```
RSS Feeds â†’ Scanner Agent â†’ Kafka Streaming â†’ Ensemble ML Models â†’ Evaluation â†’ Notifications
```

### Key Components

- **Agent Architecture**: Scanner, Ensemble, Messaging, and Autonomous Planning agents
- **Orchestration**: AWS Step Functions coordinating agent workflows
- **Streaming**: Apache Kafka (AWS MSK) for event-driven data flow
- **Vector DB**: AWS OpenSearch for similarity search and recommendations
- **Batch Processing**: Apache Spark (AWS EMR) for analytics and model training
- **Notifications**: Multi-channel delivery (Pushover, Email, SMS, WebSocket)

## ğŸ“Š System Capabilities

- **Real-time Deal Discovery**: Scan 1000+ deals/hour from RSS feeds
- **Ensemble Price Estimation**: Combine multiple ML models for accurate pricing
- **Smart Filtering**: Notify only when discount exceeds configurable threshold
- **Personalization**: AI-generated messages tailored to user preferences
- **High Availability**: 99.9% uptime with multi-AZ deployment

## ğŸ› ï¸ Technology Stack

### Backend
- **Language**: Python 3.12
- **API Framework**: FastAPI with Pydantic
- **Orchestration**: AWS Lambda + Step Functions
- **Compute**: ECS Fargate, AWS Lambda

### Data Layer
- **Streaming**: Apache Kafka (AWS MSK)
- **Batch Processing**: Apache Spark (AWS EMR)
- **Vector Database**: AWS OpenSearch with k-NN plugin
- **Relational DB**: AWS RDS Aurora PostgreSQL
- **NoSQL**: AWS DynamoDB
- **Object Storage**: AWS S3

### ML & AI
- **LLM**: AWS Bedrock (Claude 3.5 Sonnet)
- **Model Serving**: AWS SageMaker
- **Model Types**: Specialist, Frontier, Neural Network ensemble

### Frontend
- **Framework**: React.js with TypeScript
- **UI Library**: Material-UI
- **Visualization**: Plotly.js
- **Real-time**: WebSocket for live updates

### Infrastructure
- **IaC**: Terraform
- **CI/CD**: GitHub Actions / AWS CodePipeline
- **Monitoring**: Prometheus, Grafana, CloudWatch, X-Ray
- **Secrets**: AWS Secrets Manager

## ğŸ“ Repository Structure

```
dealfinder/
â”œâ”€â”€ README.md                      # This file
â”œâ”€â”€ WARP.md                        # Warp AI agent guidance
â”œâ”€â”€ project-status.md              # Detailed project timeline and status
â”œâ”€â”€ PRODUCTION_PLAN.md             # Complete production architecture plan
â”œâ”€â”€ PROCESS_FLOWS.md               # Visual workflow diagrams
â”œâ”€â”€ TECHNOLOGY_RATIONALE.md        # Technology selection reasoning
â”œâ”€â”€ .github/
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â”œâ”€â”€ ci.yml                # CI pipeline (lint, test, security)
â”‚   â”‚   â””â”€â”€ cd.yml                # CD pipeline (deploy infrastructure)
â”‚   â””â”€â”€ dependabot.yml            # Automated dependency updates
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ APACHE_ZEPPELIN.md         # Zeppelin documentation
â”‚   â””â”€â”€ developer_journal.md       # Development session logs (private)
â”œâ”€â”€ infrastructure/                # Terraform IaC âœ… DEPLOYED
â”‚   â”œâ”€â”€ bootstrap.sh              # Backend setup script
â”‚   â”œâ”€â”€ environments/
â”‚   â”‚   â””â”€â”€ dev/                  # Dev environment (live)
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ networking/           # VPC, subnets, endpoints
â”‚       â”œâ”€â”€ data/                 # S3, DynamoDB
â”‚       â””â”€â”€ monitoring/           # CloudWatch, alarms
â”œâ”€â”€ src/                           # Source code
â”‚   â””â”€â”€ dealfinder/               # Python package structure
â”œâ”€â”€ tests/                         # Test suite âœ… 97 TESTS PASSING
â”‚   â”œâ”€â”€ unit/                     # Unit tests for configs
â”‚   â””â”€â”€ infrastructure/           # Infrastructure validation
â”œâ”€â”€ socialmedia/                   # Social media content
â””â”€â”€ scripts/                       # Utility scripts (TBD)
```

## ğŸ“š Documentation

- **[project-status.md](project-status.md)**: Project timeline, phase breakdown, and current status
- **[PRODUCTION_PLAN.md](PRODUCTION_PLAN.md)**: Detailed system design, component specifications, and migration strategy
- **[PROCESS_FLOWS.md](PROCESS_FLOWS.md)**: Visual diagrams of data flows, pipelines, and workflows
- **[TECHNOLOGY_RATIONALE.md](TECHNOLOGY_RATIONALE.md)**: Reasoning behind each technology choice with alternatives considered
- **[WARP.md](WARP.md)**: Context for Warp AI agent when working in this repository
- **[infrastructure/README.md](infrastructure/README.md)**: Infrastructure deployment guide
- **[infrastructure/TERRAFORM_GUIDE.md](infrastructure/TERRAFORM_GUIDE.md)**: Terraform best practices and workflows
- **[docs/APACHE_ZEPPELIN.md](docs/APACHE_ZEPPELIN.md)**: Guide to Apache Zeppelin for interactive Spark development

## ğŸš€ Getting Started

### Prerequisites

- AWS Account with programmatic access
- Terraform 1.14+
- Python 3.12+
- uv package manager
- Git

### Development Setup

```bash
# Clone repository
git clone https://github.com/Bytes0211/dealfinder.git
cd dealfinder

# Install Python dependencies
uv pip install -e .

# Bootstrap Terraform backend (one-time)
cd infrastructure
./bootstrap.sh us-east-1 dev

# Deploy infrastructure
cd environments/dev
cp terraform.tfvars.example terraform.tfvars
# Edit terraform.tfvars with your settings
terraform init
terraform plan
terraform apply
```

### Running Tests

```bash
# Run all unit tests
pytest tests/unit/ -v

# Run infrastructure validation tests
pytest tests/infrastructure/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=html
```

## ğŸ“ˆ Performance Targets

| Metric | Target | Purpose |
|--------|--------|---------|
| **API Latency** | < 500ms (P95) | Responsive user experience |
| **Notification Delivery** | < 30 seconds | Real-time alerts |
| **Throughput** | 1000 deals/hour | Scale for multiple RSS sources |
| **Availability** | 99.9% uptime | Reliable service |
| **Error Rate** | < 0.1% | High-quality notifications |

## ğŸ’° Cost Estimation

**Estimated Monthly Costs** (1000 deals/hour, 10K users): **$1,750 - $2,900**

Major cost drivers:
- AWS MSK (Kafka): $400-600
- OpenSearch: $300-500
- ECS Fargate: $200-300
- RDS Aurora: $150-250
- AWS Bedrock: $200-400

See [TECHNOLOGY_RATIONALE.md](TECHNOLOGY_RATIONALE.md#cost-projection) for detailed breakdown.

## ğŸ—ºï¸ Roadmap

### Phase 1: Infrastructure Setup (Weeks 1-2) âœ… COMPLETE
- [x] Provision AWS accounts and VPC
- [x] Set up CI/CD pipeline  
- [x] Deploy development environment
- [x] Implement monitoring and cost controls
- [x] Create comprehensive test suite (97 tests)

### Phase 2: Data Layer (Weeks 3-4)
- [ ] Migrate ChromaDB to OpenSearch
- [ ] Set up RDS Aurora and DynamoDB
- [ ] Configure S3 data lake

### Phase 3: Application Development (Weeks 5-8)
- [ ] Implement Lambda agents
- [ ] Build FastAPI backend
- [ ] Develop React frontend
- [ ] Create Step Functions workflows

### Phase 4: Streaming Infrastructure (Weeks 9-10)
- [ ] Deploy MSK cluster
- [ ] Implement Kafka producers/consumers
- [ ] Create stream processing jobs

### Phase 5: Model Deployment (Weeks 11-12)
- [ ] Migrate to AWS Bedrock
- [ ] Deploy models to SageMaker
- [ ] Implement A/B testing

### Phase 6: Testing & Validation (Weeks 13-14)
- [ ] Integration testing
- [ ] Load testing
- [ ] Security testing

### Phase 7: Production Deployment (Weeks 15-16)
- [ ] Blue-green deployment
- [ ] Traffic migration
- [ ] Monitoring and optimization

### Phase 8: Documentation & Training (Weeks 17-18)
- [ ] Complete documentation
- [ ] Team training
- [ ] Runbooks and procedures

## ğŸ”’ Security

- **Authentication**: AWS Cognito with OAuth 2.0
- **Authorization**: IAM roles for service-to-service
- **Secrets**: AWS Secrets Manager (never hardcoded)
- **Network**: Private VPC subnets for backend services
- **Encryption**: TLS 1.3 in transit, KMS at rest
- **Compliance**: GDPR-ready with data retention policies

## ğŸ¤ Contributing

*To be added when project transitions to active development*

## ğŸ“„ License

*To be determined*

## ğŸ”— Related Projects

This system is being transformed from a Jupyter notebook prototype. Key migrations:

- **UI**: Gradio â†’ React.js + TypeScript
- **LLM**: OpenAI API â†’ AWS Bedrock (Claude)
- **Vector DB**: ChromaDB â†’ AWS OpenSearch
- **Orchestration**: Synchronous â†’ AWS Step Functions
- **Storage**: Local files â†’ S3 + DynamoDB + Aurora

## ğŸ“ Contact

*To be added*

---

**Built with** â˜ï¸ AWS | ğŸ”¥ Apache Technologies | ğŸ¤– AI/ML | âš¡ Event-Driven Architecture
